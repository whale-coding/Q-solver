package llm

import (
	"context"
	"crypto/tls"
	"encoding/json"
	"fmt"
	"net/http"
	"strings"

	openai "github.com/openai/openai-go"
	"github.com/openai/openai-go/option"
)

type OpenAIProvider struct {
	client     *openai.Client
	httpClient *http.Client
	model      string
	baseURL    string
	apiKey     string
}

func NewOpenAIProvider(apiKey, baseURL, model string) *OpenAIProvider {
	if model == "" {
		model = openai.ChatModelGPT4o
	}

	transport := &http.Transport{
		TLSClientConfig: &tls.Config{},
	}

	httpClient := &http.Client{
		Transport: transport,
	}

	opts := []option.RequestOption{
		option.WithAPIKey(apiKey),
		option.WithHTTPClient(httpClient),
	}

	if baseURL != "" {
		opts = append(opts, option.WithBaseURL(baseURL))
	}

	client := openai.NewClient(opts...)

	return &OpenAIProvider{
		client:     &client,
		httpClient: httpClient,
		model:      model,
		baseURL:    baseURL,
		apiKey:     apiKey,
	}
}

func (p *OpenAIProvider) GenerateContentStream(ctx context.Context, history []openai.ChatCompletionMessageParamUnion, onToken func(string)) (string, error) {
	stream := p.client.Chat.Completions.NewStreaming(ctx, openai.ChatCompletionNewParams{
		Model:    p.model,
		Messages: history,
	})

	defer stream.Close()

	var fullResponse strings.Builder

	for stream.Next() {
		evt := stream.Current()

		if len(evt.Choices) > 0 {
			content := evt.Choices[0].Delta.Content
			if content != "" {
				fullResponse.WriteString(content)
				if onToken != nil {
					onToken(content)
				}
			}
		}
	}

	if err := stream.Err(); err != nil {
		errStr := err.Error()

		startIndex := strings.Index(errStr, "{")
		if startIndex == -1 {
			return "", fmt.Errorf("æœªçŸ¥é”™è¯¯: %s", errStr)
		}

		jsonPart := errStr[startIndex:]
		var response struct {
			StatusCode int    `json:"statusCode"`
			Code       string `json:"code"`
			Message    string `json:"message"`
			Type       string `json:"type"`
		}

		_ = json.Unmarshal([]byte(jsonPart), &response)
		headerPart := errStr[:startIndex]

		lastColon := strings.LastIndex(headerPart, ":")

		if lastColon != -1 {
			statusPart := headerPart[lastColon+1:]
			if _, scanErr := fmt.Sscanf(statusPart, "%d", &response.StatusCode); scanErr != nil {
				response.StatusCode = 500
			}
		} else {
			response.StatusCode = -1
		}

		finalJsonBytes, marshalErr := json.Marshal(response)
		if marshalErr != nil {
			return "", fmt.Errorf("è§£æé”™è¯¯: %s", response.Message)
		}

		return "", fmt.Errorf("%s", string(finalJsonBytes))
	}

	return fullResponse.String(), nil
}

// TestChat é€šè¿‡å‘é€ç®€å•æ¶ˆæ¯æµ‹è¯•è¿é€šæ€§
func (p *OpenAIProvider) TestChat(ctx context.Context) error {
	// å‘é€ä¸€ä¸ªç®€å•çš„æ¶ˆæ¯æµ‹è¯•è¿é€šæ€§
	_, err := p.client.Chat.Completions.New(ctx, openai.ChatCompletionNewParams{
		Model: p.model,
		Messages: []openai.ChatCompletionMessageParamUnion{
			openai.UserMessage("hi"),
		},
		MaxTokens: openai.Int(1),
	})
	return err
}

func (p *OpenAIProvider) GetModels(ctx context.Context) ([]string, error) {
	resp, err := p.client.Models.List(ctx)
	if err != nil {
		return nil, err
	}
	var models []string
	for _, m := range resp.Data {
		models = append(models, m.ID)
	}
	return models, nil
}

func (p *OpenAIProvider) ParseResume(ctx context.Context, resumeBase64 string) (string, error) {
	messages := []openai.ChatCompletionMessageParamUnion{}
	messages = append(messages, openai.UserMessage(`# Role ä½ æ˜¯ä¸€ä¸ª**é€šç”¨å‹ç®€å†é‡æ„ä¸è§£æå¼•æ“**ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†è¾“å…¥çš„ç®€å†å†…å®¹ï¼ˆæ— è®ºè¡Œä¸šæ˜¯äº’è”ç½‘ã€é”€å”®ã€è´¢åŠ¡è¿˜æ˜¯è¡Œæ”¿ï¼‰ï¼Œæå–å¹¶ä¸¥æ ¼æŒ‰ç…§æˆ‘æŒ‡å®šçš„ã€é€šç”¨ç¾è§‚æ¨¡æ¿ã€‘è½¬æ¢ä¸º Raw Markdown æ ¼å¼ã€‚ # ğŸš¨ STRICT OUTPUT PROTOCOL (ç»å¯¹è¾“å‡ºåè®®) 1. **çº¯æ–‡æœ¬è¾“å‡º**ï¼šè¾“å‡º**å¿…é¡»**æ˜¯åŸå§‹ Markdown æ–‡æœ¬ã€‚**ä¸¥ç¦**ä½¿ç”¨ markdown ä»£ç å—åŒ…è£¹ã€‚ 2. **å†…å®¹è‡ªé€‚åº”**ï¼šæ ¹æ®å€™é€‰äººçš„è¡Œä¸šè°ƒæ•´å…³é”®è¯ã€‚ä¾‹å¦‚ï¼š - å¯¹äºç¨‹åºå‘˜ï¼Œæå–"æŠ€æœ¯æ ˆ"ï¼› - å¯¹äºé”€å”®ï¼Œæå–"å…³é”®å®¢æˆ·/ä¸šç»©"ï¼› - å¯¹äºè¡Œæ”¿ï¼Œæå–"åŠå…¬æŠ€èƒ½/ç»„ç»‡èƒ½åŠ›"ã€‚ 3. **ç©ºå€¼å¤„ç†**ï¼šå¦‚æœç®€å†ä¸­ç¼ºå°‘æŸé¡¹ä¿¡æ¯ï¼ˆå¦‚ä¸ªäººç½‘ç«™ï¼‰ï¼Œç›´æ¥ä¸æ˜¾ç¤ºè¯¥è¡Œã€‚ 4. **æ’ç‰ˆå¼ºåˆ¶**ï¼šä¸¥æ ¼ä¿ç•™æ¨¡æ¿ä¸­çš„ Emoji å›¾æ ‡å’Œå¼•ç”¨å—æ ¼å¼ã€‚ # ğŸ’… Universal Visual Template (é€šç”¨ç¾è§‚æ¨¡æ¿) # {{å§“å}} > ğŸ’¼ **{{æ±‚èŒæ„å‘/å½“å‰èŒä½}}** > > ğŸ“± {{ç”µè¯}}  |  ğŸ“§ {{é‚®ç®±}}  |  ğŸ“ {{æ‰€åœ¨åŸå¸‚}} > ğŸ”— [ä½œå“é›†/LinkedIn/ä¸ªäººä¸»é¡µ]({{é“¾æ¥åœ°å€}}) *(å¦‚æœ‰æ‰æ˜¾ç¤º)* --- ## âš¡ ä¸“ä¸šæŠ€èƒ½ *(è¯·æ ¹æ®èŒä¸šå±æ€§å½’ç±»ï¼Œä»¥ä¸‹ä»…ä¸ºç¤ºä¾‹ï¼Œè¯·çµæ´»è°ƒæ•´ Key)* - **æ ¸å¿ƒç«äº‰åŠ›**: {{å¦‚ï¼šå¤§å®¢æˆ·é”€å”® / è´¢åŠ¡å®¡è®¡ / Javaå¼€å‘ / å›¢é˜Ÿç®¡ç†}} - **è½¯ä»¶/å·¥å…·**: {{å¦‚ï¼šSAP / Excelé«˜çº§ / Photoshop / Docker}} - **è¯ä¹¦/è¯­è¨€**: {{å¦‚ï¼šCPAæ³¨å†Œä¼šè®¡å¸ˆ / è‹±è¯­å…­çº§ / PMP}} ## ğŸ¢ å·¥ä½œç»å† ### **{{å…¬å¸åç§°}}** **{{èŒä½åç§°}}** | *{{å¼€å§‹æ—¶é—´}} - {{ç»“æŸæ—¶é—´}}* > {{ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒèŒè´£ã€‚å¦‚ï¼šè´Ÿè´£å¤§åŒºé”€å”®å›¢é˜Ÿç®¡ç†ï¼Œæˆ–ï¼šè´Ÿè´£å…¬å¸å¹´åº¦å®¡è®¡å·¥ä½œã€‚}} - ğŸ”¸ **{{æ ¸å¿ƒä¸šç»©/äº§å‡º1}}**: {{è¯¦ç»†æè¿°ï¼Œå°½å¯èƒ½åŒ…å«æ•°æ®ï¼Œå¦‚ï¼šé”€å”®é¢å¢é•¿ 20% / èŠ‚çœæˆæœ¬ 50ä¸‡}} - ğŸ”¸ **{{æ ¸å¿ƒä¸šç»©/äº§å‡º2}}**: {{è¯¦ç»†æè¿°}} - ğŸ”¸ **{{æ ¸å¿ƒä¸šç»©/äº§å‡º3}}**: {{è¯¦ç»†æè¿°}} *(å¦‚æœæœ‰æ›´å¤šå…¬å¸ï¼Œé‡å¤ä¸Šé¢çš„æ ¼å¼)* ## ğŸ† é¡¹ç›®ä¸é‡ç‚¹ä¸šç»© *(å¦‚æœæ˜¯æŠ€æœ¯äººå‘˜å†™é¡¹ç›®ï¼›å¦‚æœæ˜¯é”€å”®å†™å¤§å®¢æˆ·æ¡ˆä¾‹ï¼›å¦‚æœæ˜¯åº”å±Šç”Ÿå†™æ ¡å›­æ´»åŠ¨)* ### ğŸ”¹ {{é¡¹ç›®/æ¡ˆä¾‹åç§°}} *{{é¡¹ç›®/æ¡ˆä¾‹ä¸€å¥è¯ç®€ä»‹}}* - **æ‰®æ¼”è§’è‰²**: {{å¦‚ï¼šé¡¹ç›®è´Ÿè´£äºº / æ ¸å¿ƒæ‰§è¡Œè€…}} - **èƒŒæ™¯/æŒ‘æˆ˜**: {{ç®€è¿°é¢ä¸´çš„é—®é¢˜}} - **æˆ‘çš„è¡ŒåŠ¨**: - {{å…·ä½“åŠ¨ä½œ 1}} - {{å…·ä½“åŠ¨ä½œ 2}} - **æœ€ç»ˆç»“æœ**: {{é‡åŒ–çš„ç»“æœ}} ## ğŸ“ æ•™è‚²ç»å† - **{{å­¦æ ¡åç§°}}** | {{ä¸“ä¸š}} | {{å­¦å†}} | *{{æ—¶é—´æ®µ}}* --- *Generated by AI Resume Assistant* # Input Data ç®€å†å†…å®¹è§é™„ä»¶ã€‚`))
	messages = append(messages, openai.UserMessage([]openai.ChatCompletionContentPartUnionParam{
		openai.ImageContentPart(openai.ChatCompletionContentPartImageImageURLParam{
			URL: "data:application/pdf;base64," + resumeBase64,
		}),
	}))
	result, err := p.GenerateContentStream(ctx, messages, nil)
	if err != nil {
		return "", err
	}
	return result, nil
}
